{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facial image emotion recognition using CNN\n",
    "# Target classes: Angry, Disgusted, Fearful, Happy, Sad, Surprised, Neutral\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# DATASET\n",
    "# files in dataset/test and dataset/train\n",
    "# subfolders: angry, disgusted, fearful, happy, sad, surprised, neutral\n",
    "\n",
    "# load dataset\n",
    "train_dir = 'dataset/train'\n",
    "test_dir = 'dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images in each class:\n",
      "angry: 3995 images\n",
      "disgusted: 436 images\n",
      "fearful: 4097 images\n",
      "happy: 7215 images\n",
      "sad: 4830 images\n",
      "surprised: 3171 images\n",
      "neutral: 4965 images\n",
      "\n",
      "Number of test images in each class:\n",
      "angry: 958 images\n",
      "disgusted: 111 images\n",
      "fearful: 1024 images\n",
      "happy: 1774 images\n",
      "sad: 1247 images\n",
      "surprised: 831 images\n",
      "neutral: 1233 images\n",
      "\n",
      "Image size: (48, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "# Check for the number of images in each class and the size of the images\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of classes\n",
    "classes = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']\n",
    "\n",
    "# Number of images in each class\n",
    "print('Number of training images in each class:')\n",
    "for c in classes:\n",
    "    path = os.path.join(train_dir, c)\n",
    "    print(f'{c}: {len(os.listdir(path))} images')\n",
    "    \n",
    "print('\\nNumber of test images in each class:')\n",
    "for c in classes:\n",
    "    path = os.path.join(test_dir, c)\n",
    "    print(f'{c}: {len(os.listdir(path))} images')\n",
    "    \n",
    "# Image size\n",
    "img = cv2.imread('dataset/train/angry/im0.png')\n",
    "print(f'\\nImage size: {img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 15:01:35.475861: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 15:01:37.270178: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/898 [=========>....................] - ETA: 1:10 - loss: 1.7794 - accuracy: 0.2716"
     ]
    }
   ],
   "source": [
    "# CNN MODEL\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# DATA PREPROCESSING\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def decode_img(img):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)  # Stelle sicher, dass deine Bilder im JPEG-Format sind\n",
    "    img = tf.image.resize(img, [48, 48])\n",
    "    return img / 255.0\n",
    "\n",
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    return parts[-2]\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "def prepare_for_training(ds, batch_size=32, cache=True, shuffle_buffer_size=1000):\n",
    "    # Diese Funktion konfiguriert das Dataset f√ºr verbesserte Leistung\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def process_labels(labels):\n",
    "    # Umwandlung von Textlabels in One-Hot-kodierte Labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    return labels\n",
    "\n",
    "train_ds = tf.data.Dataset.list_files(os.path.join(train_dir, '*/*.jpg'))\n",
    "train_ds = train_ds.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Labels verarbeiten\n",
    "labels = train_ds.map(lambda x, y: y)\n",
    "labels = process_labels(labels)\n",
    "train_ds = train_ds.map(lambda x, y: (x, process_labels(tf.expand_dims(y, axis=0))))\n",
    "\n",
    "train_ds = prepare_for_training(train_ds)\n",
    "\n",
    "test_ds = tf.data.Dataset.list_files(os.path.join(test_dir, '*/*.jpg'))\n",
    "test_ds = test_ds.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Labels verarbeiten\n",
    "test_ds = test_ds.map(lambda x, y: (x, process_labels(tf.expand_dims(y, axis=0))))\n",
    "test_ds = prepare_for_training(test_ds, cache=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "train_steps = len(os.listdir(train_dir)) // 32\n",
    "test_steps = len(os.listdir(test_dir)) // 32\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=test_ds,\n",
    "    validation_steps=test_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAINING AND VALIDATION ACCURACY\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# PLOT TRAINING AND VALIDATION LOSS\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 2])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# SAVE MODEL\n",
    "model.save('emotion_recognition_model.h5')\n",
    "print('Model saved as emotion_recognition_model.h5')\n",
    "\n",
    "# TEST MODEL\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('emotion_recognition_model.h5')\n",
    "\n",
    "# Load test image\n",
    "img = cv2.imread('dataset/test/angry/im0.png')\n",
    "img = cv2.resize(img, (48, 48))\n",
    "img = np.reshape(img, [1, 48, 48, 3])\n",
    "\n",
    "# Predict emotion\n",
    "prediction = model.predict(img)\n",
    "emotion = classes[np.argmax(prediction)]\n",
    "print(f'Predicted emotion: {emotion}')\n",
    "\n",
    "# Display image\n",
    "plt.imshow(cv2.cvtColor(img[0], cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Display prediction\n",
    "plt.bar(classes, prediction[0])\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TUDortmundML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
