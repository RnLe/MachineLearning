{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facial image emotion recognition using CNN\n",
    "# Target classes: Angry, Disgusted, Fearful, Happy, Sad, Surprised, Neutral\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# DATASET\n",
    "# files in dataset/test and dataset/train\n",
    "# subfolders: angry, disgusted, fearful, happy, sad, surprised, neutral\n",
    "\n",
    "# load dataset\n",
    "train_dir = 'dataset/train'\n",
    "test_dir = 'dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "# Check for the number of images in each class and the size of the images\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # 0 = alle Meldungen werden ausgegeben\n",
    "\n",
    "# List of classes\n",
    "classes = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']\n",
    "\n",
    "# Number of images in each class\n",
    "print('Number of training images in each class:')\n",
    "for c in classes:\n",
    "    path = os.path.join(train_dir, c)\n",
    "    print(f'{c}: {len(os.listdir(path))} images')\n",
    "    \n",
    "print('\\nNumber of test images in each class:')\n",
    "for c in classes:\n",
    "    path = os.path.join(test_dir, c)\n",
    "    print(f'{c}: {len(os.listdir(path))} images')\n",
    "    \n",
    "# Image size\n",
    "img = cv2.imread('dataset/train/angry/im0.png')\n",
    "print(f'\\nImage size: {img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN MODEL\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(48, 48, 3)),\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "import pathlib\n",
    "\n",
    "# Anzahl der Klassen\n",
    "num_classes = 7\n",
    "\n",
    "def decode_img(img):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [48, 48])\n",
    "    return img\n",
    "\n",
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    return parts[-2]\n",
    "\n",
    "def encode_label(label, label_lookup):\n",
    "    label_id = label_lookup(label)\n",
    "    # Konvertiere die Vokabulargröße in int32\n",
    "    depth = tf.cast(label_lookup.vocabulary_size(), tf.int32)\n",
    "    return tf.one_hot(label_id, depth=depth)\n",
    "\n",
    "\n",
    "# Erstelle den StringLookup Layer und passe ihn an\n",
    "def prepare_label_lookup(train_dir):\n",
    "    # Erstelle ein Dataset von Dateipfaden\n",
    "    file_paths = tf.data.Dataset.list_files(str(pathlib.Path(train_dir) / '*/*'), shuffle=False)\n",
    "    \n",
    "    # Extrahiere Labels aus den Pfaden\n",
    "    labels = file_paths.map(lambda x: tf.strings.split(x, os.path.sep)[-2])\n",
    "    \n",
    "    # Nutze den StringLookup Layer, um die Labels zu indizieren\n",
    "    label_lookup = tf.keras.layers.StringLookup(num_oov_indices=0)\n",
    "    label_lookup.adapt(labels)\n",
    "    \n",
    "    return label_lookup\n",
    "\n",
    "label_lookup = prepare_label_lookup(train_dir)\n",
    "\n",
    "\n",
    "def process_path(file_path, label_lookup):\n",
    "    label = get_label(file_path)\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    label = encode_label(label, label_lookup)\n",
    "    return img, label\n",
    "\n",
    "# Dataset-Erstellung und Mapping\n",
    "train_ds = tf.data.Dataset.list_files(str(pathlib.Path(train_dir) / '*/*.png'), shuffle=False)\n",
    "train_ds = train_ds.map(lambda x: process_path(x, label_lookup), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.list_files(str(pathlib.Path(test_dir) / '*/*.png'), shuffle=False)\n",
    "test_ds = test_ds.map(lambda x: process_path(x, label_lookup), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(640).batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(640).batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=100,\n",
    "    validation_data=test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ImageDataGenerator for training and test data\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Flow training images in batches of 32 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Flow test images in batches of 32 using test_datagen generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# TRAINING\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=test_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAINING AND VALIDATION ACCURACY\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# PLOT TRAINING AND VALIDATION LOSS\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 2])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# SAVE MODEL\n",
    "model.save('emotion_recognition_model.h5')\n",
    "print('Model saved as emotion_recognition_model.h5')\n",
    "\n",
    "# TEST MODEL\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('emotion_recognition_model.h5')\n",
    "\n",
    "# Load test image\n",
    "img = cv2.imread('dataset/test/angry/im0.png')\n",
    "img = cv2.resize(img, (48, 48))\n",
    "img = np.reshape(img, [1, 48, 48, 3])\n",
    "\n",
    "# Predict emotion\n",
    "prediction = model.predict(img)\n",
    "emotion = classes[np.argmax(prediction)]\n",
    "print(f'Predicted emotion: {emotion}')\n",
    "\n",
    "# Display image\n",
    "plt.imshow(cv2.cvtColor(img[0], cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Display prediction\n",
    "plt.bar(classes, prediction[0])\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
