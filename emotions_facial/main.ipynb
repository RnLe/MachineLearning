{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 21:45:46.737226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2024-05-05 21:45:46.872603: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-05 21:45:46.872643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 4070 Ti computeCapability: 8.9\n",
      "coreClock: 2.61GHz coreCount: 60 deviceMemorySize: 11.99GiB deviceMemoryBandwidth: 469.43GiB/s\n",
      "2024-05-05 21:45:46.872805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-05 21:45:46.873557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-05-05 21:45:46.874526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-05 21:45:46.874700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-05 21:45:46.875446: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-05-05 21:45:46.875889: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-05-05 21:45:46.877481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-05-05 21:45:46.877587: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-05 21:45:46.877606: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-05 21:45:46.877610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "# Facial image emotion recognition using CNN\n",
    "# Target classes: Angry, Disgusted, Fearful, Happy, Sad, Surprised, Neutral\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "NUM_THREADS = 32\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(NUM_THREADS)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(NUM_THREADS)\n",
    "\n",
    "# Switch to GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "print(physical_devices)\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "# DATASET\n",
    "# files in dataset/test and dataset/train\n",
    "# subfolders: angry, disgusted, fearful, happy, sad, surprised, neutral\n",
    "\n",
    "# load dataset\n",
    "train_dir = 'dataset/train'\n",
    "test_dir = 'dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images in each class:\n",
      "angry: 3995 images\n",
      "disgusted: 436 images\n",
      "fearful: 4097 images\n",
      "happy: 7215 images\n",
      "sad: 4830 images\n",
      "surprised: 3171 images\n",
      "neutral: 4965 images\n",
      "\n",
      "Number of test images in each class:\n",
      "angry: 958 images\n",
      "disgusted: 111 images\n",
      "fearful: 1024 images\n",
      "happy: 1774 images\n",
      "sad: 1247 images\n",
      "surprised: 831 images\n",
      "neutral: 1233 images\n",
      "\n",
      "Image size: (48, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "# Check for the number of images in each class and the size of the images\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # 0 = alle Meldungen werden ausgegeben\n",
    "\n",
    "# List of classes\n",
    "classes = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']\n",
    "\n",
    "# Number of images in each class\n",
    "print('Number of training images in each class:')\n",
    "for c in classes:\n",
    "    path = os.path.join(train_dir, c)\n",
    "    print(f'{c}: {len(os.listdir(path))} images')\n",
    "    \n",
    "print('\\nNumber of test images in each class:')\n",
    "for c in classes:\n",
    "    path = os.path.join(test_dir, c)\n",
    "    print(f'{c}: {len(os.listdir(path))} images')\n",
    "    \n",
    "# Image size\n",
    "img = cv2.imread('dataset/train/angry/im0.png')\n",
    "print(f'\\nImage size: {img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 11:27:08.727615: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [28709]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CacheDataset element_spec=(TensorSpec(shape=(None, 48, 48, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, None), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# CNN MODEL\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(48, 48, 3)),\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "import pathlib\n",
    "\n",
    "# Anzahl der Klassen\n",
    "num_classes = 7\n",
    "\n",
    "def decode_img(img):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [48, 48])\n",
    "    return img\n",
    "\n",
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    return parts[-2]\n",
    "\n",
    "def encode_label(label, label_lookup):\n",
    "    label_id = label_lookup(label)\n",
    "    # Konvertiere die Vokabulargröße in int32\n",
    "    depth = tf.cast(label_lookup.vocabulary_size(), tf.int32)\n",
    "    return tf.one_hot(label_id, depth=depth)\n",
    "\n",
    "\n",
    "# Erstelle den StringLookup Layer und passe ihn an\n",
    "def prepare_label_lookup(train_dir):\n",
    "    # Erstelle ein Dataset von Dateipfaden\n",
    "    file_paths = tf.data.Dataset.list_files(str(pathlib.Path(train_dir) / '*/*'), shuffle=False)\n",
    "    \n",
    "    # Extrahiere Labels aus den Pfaden\n",
    "    labels = file_paths.map(lambda x: tf.strings.split(x, os.path.sep)[-2])\n",
    "    \n",
    "    # Nutze den StringLookup Layer, um die Labels zu indizieren\n",
    "    label_lookup = tf.keras.layers.StringLookup(num_oov_indices=0)\n",
    "    label_lookup.adapt(labels)\n",
    "    \n",
    "    return label_lookup\n",
    "\n",
    "label_lookup = prepare_label_lookup(train_dir)\n",
    "\n",
    "\n",
    "def process_path(file_path, label_lookup):\n",
    "    label = get_label(file_path)\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    label = encode_label(label, label_lookup)\n",
    "    return img, label\n",
    "\n",
    "# Dataset-Erstellung und Mapping\n",
    "train_ds = tf.data.Dataset.list_files(str(pathlib.Path(train_dir) / '*/*.png'), shuffle=False)\n",
    "train_ds = train_ds.map(lambda x: process_path(x, label_lookup), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.list_files(str(pathlib.Path(test_dir) / '*/*.png'), shuffle=False)\n",
    "test_ds = test_ds.map(lambda x: process_path(x, label_lookup), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "cache_dir = 'cache'\n",
    "\n",
    "train_ds = train_ds.shuffle(640).batch(64).prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "test_ds = test_ds.shuffle(640).batch(64).prefetch(tf.data.experimental.AUTOTUNE).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_ds.take(1):  # Nimm nur das erste Batch\n",
    "    print(label)  # Zeigt die Labels des ersten Batches an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=100,\n",
    "    validation_data=test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ImageDataGenerator for training and test data\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Flow training images in batches of 32 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Flow test images in batches of 32 using test_datagen generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# TRAINING\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=test_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAINING AND VALIDATION ACCURACY\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# PLOT TRAINING AND VALIDATION LOSS\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 2])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# SAVE MODEL\n",
    "model.save('emotion_recognition_model.h5')\n",
    "print('Model saved as emotion_recognition_model.h5')\n",
    "\n",
    "# TEST MODEL\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('emotion_recognition_model.h5')\n",
    "\n",
    "# Load test image\n",
    "img = cv2.imread('dataset/test/angry/im0.png')\n",
    "img = cv2.resize(img, (48, 48))\n",
    "img = np.reshape(img, [1, 48, 48, 3])\n",
    "\n",
    "# Predict emotion\n",
    "prediction = model.predict(img)\n",
    "emotion = classes[np.argmax(prediction)]\n",
    "print(f'Predicted emotion: {emotion}')\n",
    "\n",
    "# Display image\n",
    "plt.imshow(cv2.cvtColor(img[0], cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Display prediction\n",
    "plt.bar(classes, prediction[0])\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
